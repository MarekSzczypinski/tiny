{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386153cd-684f-4bca-a6ad-eca4bcccfb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.lite.experimental.microfrontend.python.ops import audio_microfrontend_op as frontend_op\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# from tensorflow_model_optimization.python.core.quantization.keras.experimental.default_n_bit.default_n_bit_quantize_configs import (\n",
    "#     NoOpQuantizeConfig,\n",
    "# )\n",
    "# from tensorflow_model_optimization.python.core.quantization.keras.experimental.default_n_bit.default_n_bit_quantize_registry import (\n",
    "#     DefaultNBitConvQuantizeConfig,\n",
    "#     DefaultNBitQuantizeConfig,\n",
    "# )\n",
    "# from tensorflow_model_optimization.python.core.quantization.keras.experimental.default_n_bit.default_n_bit_quantize_scheme import (\n",
    "#     DefaultNBitQuantizeScheme,\n",
    "# )\n",
    "\n",
    "import functools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, pickle, sys, time\n",
    "import shutil\n",
    "import json\n",
    "from scipy.io import  wavfile\n",
    "from IPython import display\n",
    "\n",
    "import str_ww_util as util\n",
    "import get_dataset\n",
    "import keras_model as models\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6132467-f897-42fe-a324-edd03ba13f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db02f34-2ff4-43b2-b86d-0b46f3a4d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from get_dataset import decode_audio, get_label, get_waveform_and_label, \\\n",
    "#                         convert_labels_str2int, convert_to_int16, cast_and_pad, \\\n",
    "#                         convert_dataset, get_preprocess_audio_func, prepare_background_data, \\\n",
    "#                         get_data, count_labels, is_batched\n",
    "\n",
    "from get_dataset import get_data, get_file_lists, get_data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df306e-432a-4f12-a73a-cc945341dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter will pass an extra -f=<tmp_file> arg, which throws an \n",
    "# unrecognized argument error\n",
    "sys.argv = sys.argv[0:1] \n",
    "\n",
    "Flags = util.parse_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b889317-b130-4b06-97a6-93713873e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set these for an extra short test just to validate that the code runs\n",
    "Flags.num_samples_training = 2000\n",
    "Flags.num_samples_validation = -1  # val,test are not shuffled, so we'll get an unbalanced class if we take only a few\n",
    "Flags.num_samples_test = -1        # plus, they're not really that big.\n",
    "\n",
    "Flags.foreground_volume_min = 0.1\n",
    "Flags.foreground_volume_max = 1.0\n",
    "\n",
    "load_pretrained_model = True # True to load from a file, False to build/train from scratch\n",
    "save_model = False\n",
    "\n",
    "# 'trained_models/str_ww_model.h5' is the default save path for train.py\n",
    "pretrained_model_path = 'trained_models/str_ww_model.h5' # path to load from if load_pretrained_model is True\n",
    "# pretrained_model_path = '/Users/jeremy/dev/tiny_mlperf/tiny_main/trained_models/str_ww_model.h5' # path to load from if load_pretrained_model is True\n",
    "\n",
    "\n",
    "Flags.epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d4028f-8adc-46a1-ac5c-dd7f17647087",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flags.background_volume=1.0 # experimenting\n",
    "Flags.use_qat = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d86f7b-f4de-4b5e-9aed-85ddd5862714",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('streaming_config.json', 'r') as fpi:\n",
    "        streaming_config = json.load(fpi)\n",
    "    Flags.data_dir = streaming_config['speech_commands_path']\n",
    "except:\n",
    "    raise RuntimeError(\"\"\"\n",
    "        In this directory, copy streaming_config_template.json to streaming_config.json\n",
    "        and edit it to point to the directories where you have the speech commands dataset\n",
    "        and (optionally) the MUSAN noise data set.\n",
    "        \"\"\")\n",
    "Flags.bg_path = Flags.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a182fc-e9bf-4e03-a793-01d94561a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count=3\n",
    "background_frequency = Flags.background_frequency\n",
    "background_volume_range_= Flags.background_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cdbee2-1521-4bf6-bd0b-b85f41105946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dumps({'a':True, 'b':False, 'c':None})\n",
    "import importlib\n",
    "importlib.reload(get_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b1aef4-9059-440a-8174-b685ddde8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test, ds_val = get_dataset.get_all_datasets(Flags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb33f3-9951-48b3-a54b-e83855c52846",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i,(x,y) in enumerate(ds_train.unbatch()):\n",
    "  if i < 5:\n",
    "    plt.subplot(5,1,i+1)\n",
    "    plt.imshow(np.squeeze(x))\n",
    "  else:\n",
    "    max_val = np.max(x)\n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b59bf5-6f9b-4191-8696-3ed896c87473",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i,(x,y) in enumerate(ds_train.unbatch()):\n",
    "  if i < 5:\n",
    "    plt.subplot(5,1,i+1)\n",
    "    plt.imshow(np.squeeze(x))\n",
    "  else:\n",
    "    max_val = np.max(x)\n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f7176-6ce8-4a1b-b39f-96f5377b1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dat in ds_train.unbatch().take(1):\n",
    "  print(\"One element from the training set has shape:\")\n",
    "  print(f\"Input tensor shape: {dat[0].shape}\")\n",
    "  print(f\"Label shape: {dat[1].shape}\")\n",
    "  print(f\"Label : {dat[1]}\")\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec83efd-fde8-4511-9bf6-74071d3dc201",
   "metadata": {},
   "source": [
    "These next three cells can be quite slow in the current implementation, so uncomment if you want to see them.\n",
    "They\n",
    "1. Create a dataset with only targets if you want to examine those.\n",
    "2. Show spectra of some target words\n",
    "3. Count the distribution of classes in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d5894-741d-4deb-9825-390d64ac31be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tmp dataset with only the target words\n",
    "ds_only_target = ds_train.unbatch().filter(lambda x,y: (y[0] == 1.0))\n",
    "print(\"Five elements from the only-target set:\")\n",
    "for dat in ds_only_target.take(5):\n",
    "  print(f\"Input tensor shape: {dat[0].shape}\")  \n",
    "  print(f\"Label = {dat[1]}; shape = {dat[1].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791000c5-750d-4235-a27c-448b98c82c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target_examples = 3\n",
    "target_count = 0\n",
    "\n",
    "plt.Figure(figsize=(10,4))\n",
    "for dat in ds_train.unbatch():\n",
    "  # label_string = dat[1].numpy().decode('utf8')\n",
    "  if np.argmax(dat[1]) == 0:\n",
    "    target_count += 1\n",
    "    ax = plt.subplot(max_target_examples, 1, target_count)\n",
    "    # display.display(display.Audio(dat[0].numpy(), rate=16000))\n",
    "\n",
    "    log_spec = dat[0].numpy().squeeze()\n",
    "    height = log_spec.shape[0]\n",
    "    width = log_spec.shape[1]\n",
    "    X = np.linspace(0, 1.0, num=width, dtype=float)\n",
    "    Y = range(height)\n",
    "    ax.pcolormesh(X, Y, np.squeeze(log_spec))\n",
    "    if target_count >= max_target_examples:\n",
    "      break\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafe720-6a04-4ac0-bf45-8bd8456b5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at the label breakdown in the training set\n",
    "# print(count_labels(ds_train))\n",
    "\n",
    "## current count_labels is very slow.  some scratch code here towards\n",
    "## implementing a faster one by converting labels to one-hot and then summing.\n",
    "# tf.one_hot(indices, depth)\n",
    "# ds_1hot = ds_train.map(lambda dat: tf.one_hot(dat[1], 3))\n",
    "# xx = iter(ds_1hot).next()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f9bd7e-5a64-40d0-b237-243035fc5fe8",
   "metadata": {},
   "source": [
    "## Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f74e7-a735-40d1-a951-b2455030467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# during development, to reload the models module w/o restarting the kernel\n",
    "# import importlib\n",
    "# importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d88172-df6d-422b-a9ec-a47da6958ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_pretrained_model:\n",
    "  print(f\"Loading pretrained model from {pretrained_model_path}\")\n",
    "  with tfmot.quantization.keras.quantize_scope(): # needed for the QAT wrappers\n",
    "    model = keras.models.load_model(pretrained_model_path)\n",
    "else:\n",
    "  print(f\"Building model from scratch\")\n",
    "  model = models.get_model(args=Flags, use_qat=Flags.use_qat) # compile step is done inside get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6d1855-1cc3-4487-a694-525f3349389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae82dd-3138-45b0-82d9-51a53b617b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_pretrained_model:\n",
    "  callbacks = util.get_callbacks(args=Flags)\n",
    "  train_hist = model.fit(ds_train, validation_data=ds_val, callbacks=callbacks,\n",
    "                         epochs=Flags.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2f3c8-32cc-4f9e-a876-b1345f1849f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_model:\n",
    "  model.save('trained_models/str_ww_model_nb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38404fa1-9f9f-4c56-a22e-bfa99cdbcc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_pretrained_model:\n",
    "  plt.subplot(2,1,1)\n",
    "  plt.semilogy(train_hist.epoch, train_hist.history['loss'], train_hist.history['val_loss'])\n",
    "  plt.legend(['training', 'validation'])\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.subplot(2,1,2)\n",
    "  plt.plot(train_hist.epoch, train_hist.history['categorical_accuracy'], train_hist.history['val_categorical_accuracy'])\n",
    "  plt.legend(['training', 'validation'])\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af70d4-dd90-4cc1-96b7-86d79f0f1d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell can be slow with QAT enabled\n",
    "print(f\"Eval on training set\")\n",
    "model.evaluate(ds_train)\n",
    "print(f\"Eval on validation set\")\n",
    "model.evaluate(ds_val)\n",
    "print(f\"Eval on test set\")\n",
    "model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd9778-5bd9-4e25-ae50-18a93efe06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_plot_confusion_matrix(model, dataset, label_list=['marvin', 'silent', 'other']):\n",
    "  model_out = model.predict(dataset)\n",
    "  model_out = np.squeeze(model_out)\n",
    "  y_pred_val = np.argmax(model_out, axis=1)\n",
    "  \n",
    "  y_true_val = np.nan*np.zeros(y_pred_val.shape[0])\n",
    "  for i,dat in enumerate(dataset.unbatch()):\n",
    "    y_true_val[i] = np.argmax(dat[1])\n",
    "  \n",
    "  acc = sum(y_pred_val == y_true_val) / len(y_true_val)\n",
    "  print(f'Validation set accuracy: {acc:.1%}')\n",
    "\n",
    "  confusion_mtx = tf.math.confusion_matrix(y_true_val, y_pred_val) \n",
    "  plt.figure(figsize=(6, 6))\n",
    "  sns.heatmap(confusion_mtx, xticklabels=label_list, yticklabels=label_list, \n",
    "              annot=True, fmt='g')\n",
    "  plt.gca().invert_yaxis() # flip so origin is at bottom left\n",
    "  plt.xlabel('Prediction')\n",
    "  plt.ylabel('Label')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2407081-1022-4147-b6d3-d489a33f0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['marvin', 'silent', 'other']\n",
    "# model_out = model.predict(ds_val)\n",
    "# model_out = np.squeeze(model_out)\n",
    "# y_pred_val = np.argmax(model_out, axis=1)\n",
    "\n",
    "# y_true_val = np.nan*np.zeros(y_pred_val.shape[0])\n",
    "# for i,dat in enumerate(ds_val.unbatch()):\n",
    "#   y_true_val[i] = np.argmax(dat[1])\n",
    "\n",
    "# acc = sum(y_pred_val == y_true_val) / len(y_true_val)\n",
    "# print(f'Validation set accuracy: {acc:.1%}')\n",
    "\n",
    "build_and_plot_confusion_matrix(model, ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d46c6-a0a5-4c35-9d52-fb9683096e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_to_np(dset):\n",
    "  x_vals = []\n",
    "  y_vals = []\n",
    "  count = 0\n",
    "  for x,y in dset:\n",
    "    x_vals.append(x)\n",
    "    y_vals.append(y)\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "      print(f\"{count}, \", end=\"\")\n",
    "  x_vals = np.array(x_vals)\n",
    "  y_vals = np.array(y_vals)\n",
    "  print(\"\") # add a newline, since they were suppressed before\n",
    "  return x_vals, y_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d0c8c1-7494-4482-80e2-4cbe12512fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = ds_to_np(ds_train.unbatch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b006ccb-bdf0-403d-b61d-df3663bc2173",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set class distribution (marvin, silent, other): {np.sum(y_train, axis=0)}\")\n",
    "x_val, y_val = ds_to_np(ds_val.unbatch())\n",
    "print(f\"Validation set class distribution (marvin, silent, other): {np.sum(y_val, axis=0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3697f5f3-4cd5-4749-9203-79a0cacfdb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "x_train_pos = x_train[np.nonzero(y_train[:,0])]\n",
    "print(x_train_pos.shape)\n",
    "for i in range(10):\n",
    "  plt.subplot(10,1,i+1)\n",
    "  plt.imshow(np.squeeze(x_train_pos[i]).T, origin=\"lower\", aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd72136a-5314-4cb4-a96d-41f44525cd29",
   "metadata": {},
   "source": [
    "## Post-Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a3d5e5-27fc-4059-be36-6132fe073155",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_calibration_steps = 5\n",
    "tfl_file_name = \"strm_ww_int8.tflite\"\n",
    "\n",
    "# converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "if True: \n",
    "  # If we omit this block, we'll get a floating-point TFLite model,\n",
    "  # with this block, the weights and activations should be quantized to 8b integers, \n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "  ds_calibration = ds_val.unbatch().batch(1).take(num_calibration_steps)\n",
    "  def representative_dataset_gen():\n",
    "    for next_spec, label in ds_calibration:\n",
    "      yield [next_spec] \n",
    "    \n",
    "  converter.representative_dataset = representative_dataset_gen\n",
    "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # use this one\n",
    "  # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "\n",
    "  converter.inference_input_type = tf.int8  # or tf.uint8; should match dat_q in eval_quantized_model.py\n",
    "  converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "with open(tfl_file_name, \"wb\") as fpo:\n",
    "  fpo.write(tflite_quant_model)\n",
    "print(f\"Wrote to {tfl_file_name}\")\n",
    "!ls -l $tfl_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d8765-135c-45c2-8254-6de51deeec8c",
   "metadata": {},
   "source": [
    "#### Test Quantized Interpreter on One Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9667f-e5f0-4d20-a18b-91203ac8e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tfl_file_name)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape']\n",
    "output_data = []\n",
    "labels = []\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "output_scale, output_zero_point = output_details[0][\"quantization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49f1eb-539c-4df4-80ed-cfe497a501f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec, label = next(ds_val.unbatch().batch(1).take(1).as_numpy_iterator())\n",
    "\n",
    "spec_q = np.array(spec/input_scale + input_zero_point, dtype=np.int8)\n",
    "print(f\"min = {np.min(spec_q)}, max = {np.max(spec_q)}\")\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], spec_q)\n",
    "interpreter.invoke()\n",
    "out_vec = interpreter.get_tensor(output_details[0]['index'])\n",
    "pred_label = np.argmax(out_vec)\n",
    "out_vec_dequant = (out_vec.astype(np.float32) - output_zero_point)*output_scale\n",
    "\n",
    "print(f\"Output = {out_vec}.\")\n",
    "print(f\"True (vs predicted) label = {np.argmax(label)} (vs {pred_label})\")\n",
    "print(f\"Dequantized output vector = {out_vec_dequant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63f5ed-87d6-4591-a066-a8b6849a86fc",
   "metadata": {},
   "source": [
    "#### Now Measure Quantized Accuracy on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b2884-42dd-4d35-a0b7-adcd9baf4a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time(); \n",
    "predictions = []\n",
    "labels = []\n",
    "# num_test_samples = 100\n",
    "\n",
    "eval_ds = ds_val # ds_val OR ds_train OR ds_test\n",
    "\n",
    "for next_spec, next_label in eval_ds.unbatch().batch(1): # .take(num_test_samples):    \n",
    "  spec_q = np.array(next_spec/input_scale + input_zero_point, dtype=np.int8)\n",
    "  interpreter.set_tensor(input_details[0]['index'], spec_q)\n",
    "  interpreter.invoke()\n",
    "  # The function `get_tensor()` returns a copy of the tensor data.\n",
    "  # Use `tensor()` in order to get a pointer to the tensor.\n",
    "  predictions.append(np.argmax(interpreter.get_tensor(output_details[0]['index'])))\n",
    "  labels.append(next_label[0])\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "labels = np.argmax(np.array(labels), axis=1)\n",
    "num_correct = np.sum(labels == predictions)\n",
    "acc = num_correct / len(labels)\n",
    "print(f\"Accuracy = {acc:5.3f} ({num_correct}/{len(labels)})\")\n",
    "t1 = time.time(); \n",
    "print(f\"Measured validation accuracy in {t1-t0} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7100ecee-9ed6-42b4-984f-8b75ad642d11",
   "metadata": {},
   "source": [
    "As of 10 Feb 2024, the quantized accuracy on the training set is 83% and 83% on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25950850-87f2-4dda-9965-776f68c25f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(labels, predictions)\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(confusion_mtx, xticklabels=label_list, yticklabels=label_list, \n",
    "            annot=True, fmt='g')\n",
    "plt.gca().invert_yaxis() # flip so origin is at bottom left\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2750b5-4aa6-4613-bbc9-ddca78fe6cdd",
   "metadata": {},
   "source": [
    "## Run Model on Long Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cfe55f-389f-4256-b6f0-c5cc83d2a7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if streaming_config['musan_path'] is None or len(streaming_config['musan_path']) == 0:\n",
    "  raise RuntimeError(\"Stopping before the long-wave test, which requires the musan dataset in streaming_config['musan_path']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dfb8a9-f544-482c-a90b-11d9e1fee189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_and_normalize(wav_in, rel_thresh):\n",
    "  \"\"\"\n",
    "  Trims leading and trailing 'quiet' segments, where quiet is defined as \n",
    "  less than rel_thresh*max(wav_in).\n",
    "  Then scales such that RMS of trimmed wav = 1.0\n",
    "  \"\"\"\n",
    "  idx_start = np.min(np.nonzero(ww_wav > np.max(ww_wav)*rel_thresh))\n",
    "  idx_stop  = np.max(np.nonzero(ww_wav > np.max(ww_wav)*rel_thresh))\n",
    "  \n",
    "  wav_out = wav_in[idx_start:idx_stop]\n",
    "  wav_out = wav_out / np.std(wav_out) \n",
    "  return wav_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102a02f-7b4b-4643-8c54-795784b9def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flags.variable_length=True\n",
    "model_tv = models.get_model(args=Flags, use_qat=False) # Flags.use_qat)\n",
    "Flags.variable_length=False\n",
    "# transfer weights from trained model into variable-length model\n",
    "model_tv.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d4877-e789-49ab-ac73-92065a747446",
   "metadata": {},
   "source": [
    "## Build a Long Waveform for Streaming Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e9f95-ad92-49a7-b6ea-0930460c73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_thresh = 0.05 # trim out leading/trailing space with less than rel_thresh*max(waveform)\n",
    "samp_freq = Flags.sample_rate\n",
    "musan_path = streaming_config['musan_path']\n",
    "\n",
    "if True:\n",
    "    # bg_file_configs is a list of (path, start-time, stop-time, RMS level) tuples\n",
    "    bg_file_configs = [\n",
    "      (musan_path+'/speech/librivox/speech-librivox-0149.wav', 100.0, 300.0, 0.1),\n",
    "      (musan_path+'/speech/librivox/speech-librivox-0150.wav', 100.0, 300.0, 0.1),\n",
    "      (musan_path+'/speech/librivox/speech-librivox-0152.wav', 100.0, 300.0, 0.1),\n",
    "      (musan_path+'/music/hd-classical/music-hd-0002.wav', 300.0, 450.0, 0.5),\n",
    "      (musan_path+'/music/hd-classical/music-hd-0002.wav', 450.0, 600.0, 0.5),\n",
    "      (musan_path+'/speech/librivox/speech-librivox-0052.wav', 600.0, 1200.0, 0.5),\n",
    "    ]\n",
    "    long_wav_len_sec = 20*60.0\n",
    "    rng = np.random.default_rng()\n",
    "    intervals = rng.exponential(scale=7.0, size=(100))\n",
    "    # intervals < 3 sec may mess up the counting\n",
    "    intervals = intervals[intervals>3.0]\n",
    "    insertion_secs = np.cumsum(intervals[:50])\n",
    "    # This should spread the words out in time, but if the parameters\n",
    "    # are changed, then double check this.\n",
    "    insertion_secs *= 595.0/insertion_secs[-1]\n",
    "    ww_amplitudes = rng.uniform(low=0.25, high=1.0, size=(len(insertion_secs)))\n",
    "\n",
    "else:  \n",
    "    bg_file_configs = [\n",
    "      (musan_path+'/noise/free-sound/noise-free-sound-0048.wav', 0.0, 10.0, 0.5)\n",
    "    ]\n",
    "    long_wav_len_sec = 10.0\n",
    "    insertion_secs = np.array([1.0, 5.0, 8.0])\n",
    "    ww_amplitudes = np.array([1.0, 0.5, 0.25])\n",
    "\n",
    "ww_files = []\n",
    "for line in open(os.path.join(Flags.data_dir, 'testing_list.txt')):\n",
    "    if line.split('/')[0] == 'marvin':\n",
    "        ww_path = os.path.join(Flags.data_dir, line.strip())\n",
    "        ww_files.append(ww_path)\n",
    "\n",
    "ww_files = ww_files[:len(insertion_secs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f58ec7-8d4c-4ecf-aeb5-fa782ca47941",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_wav = np.zeros(int(long_wav_len_sec*samp_freq), dtype=np.float32)\n",
    "for bg_path, t_start, t_stop, rms_level in bg_file_configs: \n",
    "    bg_sampling_freq, tmp_wav = wavfile.read(bg_path)\n",
    "    assert bg_sampling_freq == samp_freq\n",
    "    tmp_wav = tmp_wav[:int((t_stop-t_start)*samp_freq)]\n",
    "    tmp_wav = rms_level*(tmp_wav / np.std(tmp_wav)) # normalize to RMS=1.0 then scale\n",
    "    long_wav[int(t_start*samp_freq):int(t_stop*samp_freq)] = tmp_wav\n",
    "\n",
    "# long_wav = long_wav * 0.0 # uncomment to zero out background noise\n",
    "\n",
    "ww_present = np.zeros(long_wav.shape)\n",
    "for i in range(len(ww_files)):\n",
    "    ww_sampling_freq, ww_wav = wavfile.read(ww_files[i])\n",
    "    assert int(ww_sampling_freq) == samp_freq\n",
    "    index = int(insertion_secs[i] * samp_freq)\n",
    "    \n",
    "    ww_wav = trim_and_normalize(ww_wav, rel_thresh)\n",
    "    assert index+len(ww_wav) < len(long_wav)\n",
    "    \n",
    "    long_wav[index:index+len(ww_wav)] += ww_amplitudes[i]*ww_wav\n",
    "    ww_present[index:index+len(ww_wav)] = 1\n",
    "\n",
    "    # ww_windows.append((index,index+len(ww_wav)))\n",
    "\n",
    "data_config_long = get_data_config(Flags, 'training')\n",
    "data_config_long['foreground_volume_max'] = data_config_long['foreground_volume_min'] = 1.0 # scale to [-1.0,1.0]\n",
    "data_config_long['background_frequency'] = 0.0 # do not add background noise or time-shift the input\n",
    "data_config_long['time_shift_ms'] = 0.0\n",
    "data_config_long['desired_samples']= len(long_wav)\n",
    "\n",
    "long_wav = long_wav / np.max(np.abs(long_wav)) # scale into [-1.0, +1.0] range\n",
    "t = np.arange(len(long_wav))/samp_freq\n",
    "\n",
    "feature_extractor_long = get_dataset.get_preprocess_audio_func(data_config_long)\n",
    "# the feature extractor needs a label (in 1-hot format), but it doesn't matter what it is\n",
    "long_spec = feature_extractor_long({'audio':long_wav, 'label':[0.0, 0.0, 0.0]})['audio'].numpy()\n",
    "\n",
    "print(f\"Long waveform shape = {long_wav.shape}, spectrogram shape = {long_spec.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5744edd-7e27-41e4-8523-cb75d3d7907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## scale wav into [-32768, +32767] and cast to int16 for wav file\n",
    "wavfile.write('long_wav.wav', 16000, (long_wav*(2**15)).astype(np.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba66f636-cd87-4f23-94ff-4e1ffd35bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = model_tv(np.expand_dims(long_spec, 0))[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa28c391-7862-4e6e-84ae-85b899c6bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(long_wav.shape)\n",
    "print(ww_present.shape)\n",
    "importlib.reload(util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3aeb47-1cd3-4df3-a0fc-f1e1243322f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_thresh = 0.95\n",
    "## shows detection when wakeword activation is strongest output\n",
    "# ww_detected_spec_scale = (np.argmax(yy, axis=1)==0) # detections on the time scale of spectrograms\n",
    "\n",
    "## shows detection when ww activation > thresh\n",
    "ww_detected_spec_scale = (yy[:,0]>det_thresh).astype(int)\n",
    "ww_true_detects, ww_false_detects, ww_false_rejects = util.get_true_and_false_detections(ww_detected_spec_scale, ww_present, Flags)\n",
    "\n",
    "print(f\"{np.sum(ww_false_detects!=0)} false detections by counting\")\n",
    "print(f\"{np.sum(ww_true_detects!=0)} true detections by counting\")\n",
    "print(f\"{np.sum(ww_false_rejects!=0)} false rejections by counting\")\n",
    "\n",
    "plt.plot(t, 1*util.zero2nan(ww_false_detects),   'rx', label='False Detections')\n",
    "plt.plot(t, 2*(util.zero2nan(ww_true_detects)),  'g*', label='True Detections')\n",
    "plt.plot(t, 3*(util.zero2nan(ww_false_rejects)), 'bo', label='False Rejections')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c516d7-fd04-43e6-bfb2-0c92736f7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_clips_to_show = np.minimum(5, np.sum(ww_false_detects, dtype=np.int32))\n",
    "for i in range(fp_clips_to_show):\n",
    "  fp_start = np.nonzero(ww_false_detects)[0][i]\n",
    "  print(f\"False positive at {fp_start/samp_freq:3.2f}s (sample {fp_start})\")\n",
    "  fp_clip = slice(fp_start-32000,fp_start+32000)\n",
    "  display.display(display.Audio(long_wav[fp_clip], rate=16000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ea3db-f803-4427-9efa-10c565f4eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_clips_to_show = np.minimum(5, np.sum(ww_false_rejects, dtype=np.int32))\n",
    "for i in range(fp_clips_to_show):\n",
    "  fn_start = np.nonzero(ww_false_rejects)[0][i]\n",
    "  print(f\"False negative at {fn_start/samp_freq:3.2f}s (sample {fn_start})\")\n",
    "  fn_clip = slice(fn_start-16000,fn_start+16000)\n",
    "  display.display(display.Audio(long_wav[fn_clip], rate=16000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ea57c-8bbc-4cd4-9860-782b1257c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def examine_clip(wav_clip, model, feature_extractor):\n",
    "    clip_spec = feature_extractor({'audio':wav_clip, 'label':[0.0, 0.0, 0.0]})['audio'].numpy()\n",
    "    spec_len = int(1.1*len(wav_clip)/(Flags.sample_rate*Flags.window_stride_ms/1000))\n",
    "    clip_spec = clip_spec[:spec_len,:]\n",
    "    yy_clip = model(np.expand_dims(clip_spec, 0))[0].numpy()\n",
    "\n",
    "    plt.subplot(3,1,1)\n",
    "    t_wav= np.arange(len(wav_clip))/Flags.sample_rate\n",
    "    plt.plot(t_wav, wav_clip)\n",
    "                     \n",
    "    plt.subplot(3,1,2)\n",
    "    # plt.imshow(clip_spec.squeeze().T, origin=\"lower\", aspect=\"auto\");\n",
    "    t_spec= np.arange(clip_spec.shape[0])*(Flags.window_stride_ms/1000)\n",
    "    mels = np.arange(clip_spec.shape[-1])\n",
    "    plt.pcolormesh(t_spec, mels, clip_spec.squeeze().T)\n",
    "  \n",
    "    plt.subplot(3,1,3)\n",
    "    t_yy= np.arange(yy_clip.shape[0])*(Flags.window_stride_ms/1000)\n",
    "    print(f\"t_yy shape = {t_yy.shape}, yy_clip shape = {yy_clip.shape}\")\n",
    "    plt.plot(t_yy, yy_clip);\n",
    "    plt.legend(label_list, loc='lower right', fontsize=8);\n",
    "   \n",
    "    display.display(display.Audio(wav_clip, rate=16000))\n",
    "    plt.tight_layout()\n",
    "\n",
    "clip_idx = 1860383\n",
    "clip_range = slice(clip_idx-16000,clip_idx+16000)\n",
    "wav_clip = long_wav[clip_range]\n",
    "examine_clip(wav_clip, model_tv, feature_extractor_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e3439-cacc-40ef-a063-78ba69e3e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_spec= np.arange(long_spec.shape[0])*(Flags.window_stride_ms/1000)\n",
    "\n",
    "ww_detected = np.repeat(ww_detected_spec_scale, Flags.window_stride_ms*Flags.sample_rate/1000)\n",
    "extra_zeros = np.zeros(len(long_wav)-len(ww_detected))\n",
    "print(f\"added {len(extra_zeros)} extra zeros\")\n",
    "ww_detected = np.concatenate((extra_zeros, ww_detected), axis=0)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.imshow(np.squeeze(long_spec).T, origin=\"lower\", aspect='auto')\n",
    "\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(t, long_wav, t, ww_present, t, 1.1*ww_detected)\n",
    "plt.xlim([np.min(t), np.max(t)])\n",
    "plt.grid(True)\n",
    "plt.legend(['Waveform', 'Wakeword Present', 'Wakeword Detected'], loc='lower right', fontsize=8)\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(t_spec[29:], yy[:])\n",
    "plt.legend(label_list, loc='lower right', fontsize=8);\n",
    "# display.display(display.Audio(long_wav, rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19666bc5-4b43-41ef-b93d-050ff7f87dd0",
   "metadata": {},
   "source": [
    "## Quantized Model on Long Wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde0e4f-c30e-45c1-aa4f-8afe5b3ef76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_q = np.nan*np.zeros((long_spec.shape[0]-model.input.shape[1]+1,3))\n",
    "t0 = time.time(); \n",
    "preds_q = []\n",
    "labels = []\n",
    "\n",
    "for idx in range(long_spec.shape[0]-model.input.shape[1]+1):\n",
    "  spec = long_spec[idx:idx+input_shape[1],:,:]\n",
    "  spec = np.expand_dims(spec, 0) # add batch dimension  \n",
    "  spec_q = np.array(spec/input_scale + input_zero_point, dtype=np.int8)\n",
    "  \n",
    "  interpreter.set_tensor(input_details[0]['index'], spec_q)\n",
    "  interpreter.invoke()\n",
    "  # The function `get_tensor()` returns a copy of the tensor data.\n",
    "  # Use `tensor()` in order to get a pointer to the tensor.\n",
    "  yy_q[idx,:] = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Dequantize so that softmax output is in range [0,1]\n",
    "yy_q = (yy_q.astype(np.float32) - output_zero_point)*output_scale\n",
    "\n",
    "t1 = time.time(); \n",
    "print(f\"Ran quantized model on long wave in {t1-t0:1.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbd6f83-22d5-463f-ae1c-1e49a2d2486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_thresh = 0.9\n",
    "## shows detection when wakeword activation is strongest output\n",
    "# ww_detected_spec_scale = (np.argmax(yy, axis=1)==0) # detections on the time scale of spectrograms\n",
    "\n",
    "## shows detection when ww activation > thresh\n",
    "ww_detected_spec_scale = (yy_q[:,0]>det_thresh).astype(int)\n",
    "\n",
    "ww_true_detects, ww_false_detects, ww_false_rejects = util.get_true_and_false_detections(ww_detected_spec_scale, ww_present, Flags)\n",
    "\n",
    "print(f\"{np.sum(ww_false_detects!=0)} false detections by counting\")\n",
    "print(f\"{np.sum(ww_true_detects!=0)} true detections by counting\")\n",
    "print(f\"{np.sum(ww_false_rejects!=0)} false rejections by counting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89671bb6-294b-4ead-949d-d38891b2a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, 1*util.zero2nan(ww_false_detects),   'rx', label='False Detections')\n",
    "plt.plot(t, 2*(util.zero2nan(ww_true_detects)),  'g*', label='True Detections')\n",
    "plt.plot(t, 3*(util.zero2nan(ww_false_rejects)), 'bs', label='False Rejections')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419ef55-1132-425c-91b9-534bfe7679de",
   "metadata": {},
   "source": [
    "## Scratch Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62bb83c-2495-4cb0-b7c2-b337ef3905d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise RuntimeError(\"Not an error -- just stop here\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
