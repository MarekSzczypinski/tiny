{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386153cd-684f-4bca-a6ad-eca4bcccfb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.lite.experimental.microfrontend.python.ops import audio_microfrontend_op as frontend_op\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "from tensorflow_model_optimization.python.core.quantization.keras.experimental.default_n_bit.default_n_bit_quantize_configs import (\n",
    "    NoOpQuantizeConfig,\n",
    ")\n",
    "from tensorflow_model_optimization.python.core.quantization.keras.experimental.default_n_bit.default_n_bit_quantize_registry import (\n",
    "    DefaultNBitConvQuantizeConfig,\n",
    "    DefaultNBitQuantizeConfig,\n",
    ")\n",
    "from tensorflow_model_optimization.python.core.quantization.keras.experimental.default_n_bit.default_n_bit_quantize_scheme import (\n",
    "    DefaultNBitQuantizeScheme,\n",
    ")\n",
    "\n",
    "import functools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, pickle, sys\n",
    "import shutil\n",
    "import json\n",
    "from scipy.io import  wavfile\n",
    "from IPython import display\n",
    "\n",
    "import str_ww_util as util\n",
    "import get_dataset\n",
    "import keras_model as models\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6132467-f897-42fe-a324-edd03ba13f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db02f34-2ff4-43b2-b86d-0b46f3a4d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from get_dataset import decode_audio, get_label, get_waveform_and_label, \\\n",
    "#                         convert_labels_str2int, convert_to_int16, cast_and_pad, \\\n",
    "#                         convert_dataset, get_preprocess_audio_func, prepare_background_data, \\\n",
    "#                         get_data, count_labels, is_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df306e-432a-4f12-a73a-cc945341dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter will pass an extra -f=<tmp_file> arg, which throws an \n",
    "# unrecognized argument error\n",
    "sys.argv = sys.argv[0:1] \n",
    "\n",
    "Flags = util.parse_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b889317-b130-4b06-97a6-93713873e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set these for an extra short test just to validate that the code runs\n",
    "Flags.num_samples_training = -1\n",
    "Flags.num_samples_validation = -1\n",
    "Flags.num_samples_test = -1\n",
    "\n",
    "Flags.foreground_volume_min = 0.1\n",
    "Flags.foreground_volume_max = 1.0\n",
    "\n",
    "load_pretrained_model = True # True to load from a file, False to build/train from scratch\n",
    "save_model = False\n",
    "\n",
    "# 'trained_models/str_ww_model.h5' is the default save path for train.py\n",
    "pretrained_model_path = 'trained_models/str_ww_model.h5' # path to load from if load_pretrained_model is True\n",
    "\n",
    "Flags.epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d4028f-8adc-46a1-ac5c-dd7f17647087",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flags.background_volume=1.0 # experimenting\n",
    "Flags.use_qat = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d86f7b-f4de-4b5e-9aed-85ddd5862714",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('streaming_config.json', 'r') as fpi:\n",
    "        streaming_config = json.load(fpi)\n",
    "    Flags.data_dir = streaming_config['speech_commands_path']\n",
    "except:\n",
    "    raise RuntimeError(\"\"\"\n",
    "        In this directory, copy streaming_config_template.json to streaming_config.json\n",
    "        and edit it to point to the directories where you have the speech commands dataset\n",
    "        and (optionally) the MUSAN noise data set.\n",
    "        \"\"\")\n",
    "Flags.bg_path = Flags.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a182fc-e9bf-4e03-a793-01d94561a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_count=3\n",
    "background_frequency = Flags.background_frequency\n",
    "background_volume_range_= Flags.background_volume\n",
    "model_settings = models.prepare_model_settings(label_count, Flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff200e-5a19-41f8-873f-fdeb8a962264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dumps({'a':True, 'b':False, 'c':None})\n",
    "Flags.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b1aef4-9059-440a-8174-b685ddde8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test, ds_val = get_dataset.get_all_datasets(Flags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f7176-6ce8-4a1b-b39f-96f5377b1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dat in ds_train.unbatch().take(1):\n",
    "  print(\"One element from the training set has shape:\")\n",
    "  print(f\"Input tensor shape: {dat[0].shape}\")\n",
    "  print(f\"Label shape: {dat[1].shape}\")\n",
    "  print(f\"Label : {dat[1]}\")\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec83efd-fde8-4511-9bf6-74071d3dc201",
   "metadata": {},
   "source": [
    "These next three cells can be quite slow in the current implementation, so uncomment if you want to see them.\n",
    "They\n",
    "1. Create a dataset with only targets if you want to examine those.\n",
    "2. Show spectra of some target words\n",
    "3. Count the distribution of classes in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132149a-78d5-4a25-954d-a11a1269ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in ds_train.unbatch().take(1):\n",
    "  print('Hi!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d5894-741d-4deb-9825-390d64ac31be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tmp dataset with only the target words\n",
    "ds_only_target = ds_train.unbatch().filter(lambda x,y: (y[0] == 1.0))\n",
    "print(\"Five elements from the only-target set:\")\n",
    "for dat in ds_only_target.take(5):\n",
    "  print(f\"Input tensor shape: {dat[0].shape}\")  \n",
    "  print(f\"Label = {dat[1]}; shape = {dat[1].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791000c5-750d-4235-a27c-448b98c82c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target_examples = 3\n",
    "target_count = 0\n",
    "\n",
    "plt.Figure(figsize=(10,4))\n",
    "for dat in ds_train.unbatch():\n",
    "  # label_string = dat[1].numpy().decode('utf8')\n",
    "  if np.argmax(dat[1]) == 0:\n",
    "    target_count += 1\n",
    "    ax = plt.subplot(max_target_examples, 1, target_count)\n",
    "    # display.display(display.Audio(dat[0].numpy(), rate=16000))\n",
    "\n",
    "    log_spec = dat[0].numpy()\n",
    "    height = log_spec.shape[0]\n",
    "    width = log_spec.shape[1]\n",
    "    X = np.linspace(0, 1.0, num=width, dtype=float)\n",
    "    Y = range(height)\n",
    "    ax.pcolormesh(X, Y, np.squeeze(log_spec))\n",
    "    if target_count >= max_target_examples:\n",
    "      break\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c5e28-2a46-4508-9828-0b477d0828b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafe720-6a04-4ac0-bf45-8bd8456b5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at the label breakdown in the training set\n",
    "# print(count_labels(ds_train))\n",
    "\n",
    "## current count_labels is very slow.  some scratch code here towards\n",
    "## implementing a faster one by converting labels to one-hot and then summing.\n",
    "# tf.one_hot(indices, depth)\n",
    "# ds_1hot = ds_train.map(lambda dat: tf.one_hot(dat[1], 3))\n",
    "# xx = iter(ds_1hot).next()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f9bd7e-5a64-40d0-b237-243035fc5fe8",
   "metadata": {},
   "source": [
    "## Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f74e7-a735-40d1-a951-b2455030467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# during development, to reload the models module w/o restarting the kernel\n",
    "# import importlib\n",
    "# importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d88172-df6d-422b-a9ec-a47da6958ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_pretrained_model:\n",
    "  print(f\"Loading pretrained model from {pretrained_model_path}\")\n",
    "  with tfmot.quantization.keras.quantize_scope(): # needed for the QAT wrappers\n",
    "    model = keras.models.load_model(pretrained_model_path)\n",
    "else:\n",
    "  print(f\"Building model from scratch\")\n",
    "  model = models.get_model(args=Flags, use_qat=Flags.use_qat) # compile step is done inside get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6d1855-1cc3-4487-a694-525f3349389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae82dd-3138-45b0-82d9-51a53b617b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_pretrained_model:\n",
    "  callbacks = util.get_callbacks(args=Flags)\n",
    "  train_hist = model.fit(ds_train, validation_data=ds_val, callbacks=callbacks,\n",
    "                         epochs=Flags.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc55769d-5c0c-478f-a345-29fd77d44dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2f3c8-32cc-4f9e-a876-b1345f1849f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_model:\n",
    "  model.save('trained_models/str_ww_model_nb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38404fa1-9f9f-4c56-a22e-bfa99cdbcc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_pretrained_model:\n",
    "  plt.subplot(2,1,1)\n",
    "  plt.semilogy(train_hist.epoch, train_hist.history['loss'], train_hist.history['val_loss'])\n",
    "  plt.legend(['training', 'validation'])\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.subplot(2,1,2)\n",
    "  plt.plot(train_hist.epoch, train_hist.history['categorical_accuracy'], train_hist.history['val_categorical_accuracy'])\n",
    "  plt.legend(['training', 'validation'])\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af70d4-dd90-4cc1-96b7-86d79f0f1d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell can be slow with QAT enabled\n",
    "print(f\"Eval on training set\")\n",
    "model.evaluate(ds_train)\n",
    "print(f\"Eval on validation set\")\n",
    "model.evaluate(ds_val)\n",
    "print(f\"Eval on test set\")\n",
    "model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2407081-1022-4147-b6d3-d489a33f0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['marvin', 'silent', 'other']\n",
    "model_out = model.predict(ds_val)\n",
    "model_out = np.squeeze(model_out)\n",
    "y_pred_val = np.argmax(model_out, axis=1)\n",
    "\n",
    "y_true_val = np.nan*np.zeros(y_pred_val.shape[0])\n",
    "for i,dat in enumerate(ds_val.unbatch()):\n",
    "  y_true_val[i] = np.argmax(dat[1])\n",
    "\n",
    "acc = sum(y_pred_val == y_true_val) / len(y_true_val)\n",
    "print(f'Validation set accuracy: {acc:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f33291-3f5b-4631-a384-4adb37304589",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(y_true_val, y_pred_val) \n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(confusion_mtx, xticklabels=label_list, yticklabels=label_list, \n",
    "            annot=True, fmt='g')\n",
    "plt.gca().invert_yaxis() # flip so origin is at bottom left\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d46c6-a0a5-4c35-9d52-fb9683096e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_to_np(dset):\n",
    "  x_vals = []\n",
    "  y_vals = []\n",
    "  count = 0\n",
    "  for x,y in dset:\n",
    "    x_vals.append(x)\n",
    "    y_vals.append(y)\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "      print(f\"{count}, \", end=\"\")\n",
    "  x_vals = np.array(x_vals)\n",
    "  y_vals = np.array(y_vals)\n",
    "  return x_vals, y_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d0c8c1-7494-4482-80e2-4cbe12512fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = ds_to_np(ds_train.unbatch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3697f5f3-4cd5-4749-9203-79a0cacfdb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "x_train_pos = x_train[np.nonzero(y_train[:,0])]\n",
    "print(x_train_pos.shape)\n",
    "for i in range(10):\n",
    "  plt.subplot(10,1,i+1)\n",
    "  plt.imshow(np.squeeze(x_train_pos[i]).T, origin=\"lower\", aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd72136a-5314-4cb4-a96d-41f44525cd29",
   "metadata": {},
   "source": [
    "## Post-Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a3d5e5-27fc-4059-be36-6132fe073155",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_calibration_steps = 5\n",
    "tfl_file_name = \"strm_ww_int8.tflite\"\n",
    "\n",
    "# converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "if True: \n",
    "  # If we omit this block, we'll get a floating-point TFLite model,\n",
    "  # with this block, the weights and activations should be quantized to 8b integers, \n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "  ds_calibration = ds_val.unbatch().batch(1).take(num_calibration_steps)\n",
    "  def representative_dataset_gen():\n",
    "    for next_spec, label in ds_calibration:\n",
    "      yield [next_spec] \n",
    "    \n",
    "  converter.representative_dataset = representative_dataset_gen\n",
    "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # use this one\n",
    "  # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "\n",
    "  converter.inference_input_type = tf.int8  # or tf.uint8; should match dat_q in eval_quantized_model.py\n",
    "  converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "with open(tfl_file_name, \"wb\") as fpo:\n",
    "  fpo.write(tflite_quant_model)\n",
    "print(f\"Wrote to {tfl_file_name}\")\n",
    "!ls -l $tfl_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d8765-135c-45c2-8254-6de51deeec8c",
   "metadata": {},
   "source": [
    "#### Test Quantized Interpreter on One Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9667f-e5f0-4d20-a18b-91203ac8e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tfl_file_name)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape']\n",
    "output_data = []\n",
    "labels = []\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49f1eb-539c-4df4-80ed-cfe497a501f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec, label = next(ds_val.unbatch().batch(1).take(1).as_numpy_iterator())\n",
    "\n",
    "spec_q = np.array(spec/input_scale + input_zero_point, dtype=np.int8)\n",
    "print(f\"min = {np.min(spec_q)}, max = {np.max(spec_q)}\")\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], spec_q)\n",
    "interpreter.invoke()\n",
    "out_vec = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(out_vec)\n",
    "pred_label = np.argmax(out_vec[0])\n",
    "print(f\"True label = {label}. Output = {out_vec}.  Predicted label = {pred_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63f5ed-87d6-4591-a066-a8b6849a86fc",
   "metadata": {},
   "source": [
    "#### Now Measure Quantized Accuracy on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b2884-42dd-4d35-a0b7-adcd9baf4a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "labels = []\n",
    "# num_test_samples = 100\n",
    "\n",
    "eval_ds = ds_val # ds_val OR ds_train OR ds_test\n",
    "\n",
    "for next_spec, next_label in eval_ds.unbatch().batch(1): # .take(num_test_samples):    \n",
    "  spec_q = np.array(next_spec/input_scale + input_zero_point, dtype=np.int8)\n",
    "  \n",
    "  interpreter.set_tensor(input_details[0]['index'], spec_q)\n",
    "  interpreter.invoke()\n",
    "  # The function `get_tensor()` returns a copy of the tensor data.\n",
    "  # Use `tensor()` in order to get a pointer to the tensor.\n",
    "  predictions.append(np.argmax(interpreter.get_tensor(output_details[0]['index'])))\n",
    "  labels.append(next_label[0])\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "labels = np.argmax(np.array(labels), axis=1)\n",
    "num_correct = np.sum(labels == predictions)\n",
    "acc = num_correct / len(labels)\n",
    "print(f\"Accuracy = {acc:5.3f} ({num_correct}/{len(labels)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7100ecee-9ed6-42b4-984f-8b75ad642d11",
   "metadata": {},
   "source": [
    "As of 10 Feb 2024, the quantized accuracy on the training set is 83% and 83% on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25950850-87f2-4dda-9965-776f68c25f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(labels, predictions)\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(confusion_mtx, xticklabels=label_list, yticklabels=label_list, \n",
    "            annot=True, fmt='g')\n",
    "plt.gca().invert_yaxis() # flip so origin is at bottom left\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2750b5-4aa6-4613-bbc9-ddca78fe6cdd",
   "metadata": {},
   "source": [
    "## Run Model on Long Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cfe55f-389f-4256-b6f0-c5cc83d2a7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if streaming_config['musan_path'] is None or len(streaming_config['musan_path']) == 0:\n",
    "  raise RuntimeError(\"Stopping before the long-wave test, which requires the musan dataset in streaming_config['musan_path']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dfb8a9-f544-482c-a90b-11d9e1fee189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_and_normalize(wav_in, rel_thresh):\n",
    "  \"\"\"\n",
    "  Trims leading and trailing 'quiet' segments, where quiet is defined as \n",
    "  less than rel_thresh*max(wav_in).\n",
    "  Then scales such that RMS of trimmed wav = 1.0\n",
    "  \"\"\"\n",
    "  idx_start = np.min(np.nonzero(ww_wav > np.max(ww_wav)*rel_thresh))\n",
    "  idx_stop  = np.max(np.nonzero(ww_wav > np.max(ww_wav)*rel_thresh))\n",
    "  \n",
    "  wav_out = wav_in[idx_start:idx_stop]\n",
    "  wav_out = wav_out / np.std(wav_out) \n",
    "  return wav_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c849bcf-9372-4fb2-81d3-5c670265356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102a02f-7b4b-4643-8c54-795784b9def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flags.variable_length=True\n",
    "model_tv = models.get_model(args=Flags, use_qat=False) # Flags.use_qat)\n",
    "Flags.variable_length=False\n",
    "# transfer weights from trained model into variable-length model\n",
    "model_tv.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e9f95-ad92-49a7-b6ea-0930460c73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_thresh = 0.05 # trim out leading/trailing space with less than rel_thresh*max(waveform)\n",
    "\n",
    "model_settings = models.prepare_model_settings(label_count, Flags)\n",
    "samp_freq = model_settings['sample_rate']\n",
    "\n",
    "background_wav_file = os.path.join(streaming_config['musan_path'], \n",
    "                                   'noise/free-sound/noise-free-sound-0048.wav'\n",
    "                                  )\n",
    "\n",
    "\n",
    "ww_files = ['marvin/cce7416f_nohash_0.wav',\n",
    "            'marvin/321aba74_nohash_0.wav',\n",
    "            'marvin/fcb25a78_nohash_0.wav'\n",
    "           ]\n",
    "\n",
    "insertion_secs = [1.5, 3.5, 5.0]\n",
    "snr_levels = [2.0, 1.0, 0.5]\n",
    "# snr_levels = [20.0, 5.0, 1.0]\n",
    "# snr_levels = [1.0, 1.0, 1.0]\n",
    "\n",
    "for i in range(len(ww_files)):\n",
    "  ww_files[i] = os.path.join(Flags.data_dir, ww_files[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27eca95-f469-424d-95f1-dcd60b4fb3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import get_dataset\n",
    "importlib.reload(get_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f58ec7-8d4c-4ecf-aeb5-fa782ca47941",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_sampling_freq, long_wav = wavfile.read(background_wav_file)\n",
    "assert bg_sampling_freq == samp_freq\n",
    "long_wav = long_wav / np.std(long_wav) # normalize to RMS = 1.0\n",
    "\n",
    "long_wav = long_wav[0:10*samp_freq]\n",
    "\n",
    "# long_wav = long_wav * 0.0 # uncomment to zero out background noise\n",
    "\n",
    "ww_present = np.nan*np.zeros(long_wav.shape)\n",
    "\n",
    "for i in range(len(ww_files)):\n",
    "  ww_sampling_freq, ww_wav = wavfile.read(ww_files[i])\n",
    "  assert int(ww_sampling_freq) == samp_freq\n",
    "  index = int(insertion_secs[i] * samp_freq)\n",
    "  \n",
    "  ww_wav = trim_and_normalize(ww_wav, rel_thresh)\n",
    "  assert index+len(ww_wav) < len(long_wav)\n",
    "  \n",
    "  long_wav[index:index+len(ww_wav)] += snr_levels[i]*ww_wav\n",
    "  ww_present[index:index+len(ww_wav)] = 1\n",
    "\n",
    "model_settings_long = model_settings.copy()\n",
    "# scaling of the waveform by fg_vol_min/max is gated by is_training=true\n",
    "model_settings_long['desired_samples']= len(long_wav)\n",
    "model_settings_long['foreground_volume_max'] = model_settings_long['foreground_volume_min'] = 1.0\n",
    "model_settings_long['background_frequency']= 0.0\n",
    "\n",
    "\n",
    "long_wav = long_wav / np.max(np.abs(long_wav)) # scale into [-1.0, +1.0] range\n",
    "feature_extractor_long = get_dataset.get_preprocess_audio_func(model_settings_long, is_training=True)\n",
    "# the feature extractor needs a label, but it doesn't matter what it is\n",
    "long_spec = feature_extractor_long({'audio':long_wav, 'label':[0.0, 0.0, 0.0]})['audio'].numpy()\n",
    "\n",
    "print(f\"Long waveform shape = {long_wav.shape}, spectrogram shape = {long_spec.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53183428-883a-4dd7-9670-8f691194e7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,5))\n",
    "# peak_vals = []\n",
    "# sc_vals = np.linspace(0.1, 1.0, 4)\n",
    "# for i,kk in enumerate(sc_vals):\n",
    "#   model_settings_long['foreground_volume_max'] = model_settings_long['foreground_volume_min'] = kk\n",
    "#   feature_extractor_long = get_dataset.get_preprocess_audio_func(model_settings_long, is_training=True)\n",
    "#   long_spec = feature_extractor_long({'audio':long_wav})['audio'].numpy()\n",
    "#   peak_vals.append(np.max(np.abs(long_spec)))\n",
    "#   plt.subplot(4,1,i+1)\n",
    "#   plt.imshow(np.squeeze(long_spec).T, origin=\"lower\", aspect='auto')\n",
    "#   plt.colorbar()\n",
    "#   plt.ylabel(f'sc={kk}')\n",
    "                   \n",
    "# plt.plot(sc_vals, np.array(peak_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5744edd-7e27-41e4-8523-cb75d3d7907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## scale wav into [-32768, +32767] and cast to int16 for wav file\n",
    "wavfile.write('long_wav.wav', 16000, (long_wav*(2**15)).astype(np.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e3439-cacc-40ef-a063-78ba69e3e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplot(3,1,1)\n",
    "plt.imshow(np.squeeze(long_spec).T, origin=\"lower\", aspect='auto')\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "t = np.arange(len(long_wav))/samp_freq\n",
    "plt.plot(t, long_wav, t, ww_present)\n",
    "plt.xlim([np.min(t), np.max(t)])\n",
    "plt.legend(['Waveform', 'Wakeword Present'], loc='lower right')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "yy = model_tv(np.expand_dims(long_spec, 0))[0].numpy()\n",
    "# plt.plot(yy)\n",
    "plt.plot(yy[:])\n",
    "plt.legend(label_list, loc='lower right')\n",
    "display.display(display.Audio(long_wav, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f69208-d092-424d-b6ee-e00e3dd6cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError(\"Not an error -- just stop here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419ef55-1132-425c-91b9-534bfe7679de",
   "metadata": {},
   "source": [
    "## Scratch Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12bf1a-d5fe-42cb-aad0-16a09c89e3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
